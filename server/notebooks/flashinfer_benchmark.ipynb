{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc6c9d6-6985-4d01-b275-ba7166d1dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "import flashinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4cd586-0e7b-42a7-b4cf-67b86e43a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_head = 2\n",
    "head_dim = 128\n",
    "page_len = 16\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e528617-c808-426b-b593-e2b6249af3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a, b):\n",
    "    rtol, atol = {\n",
    "        torch.float16: (1e-3, 5e-4),\n",
    "        torch.float32: (1e-5, 5e-6),\n",
    "        torch.bfloat16: (8e-3, 8e-3),\n",
    "    }[a.dtype]\n",
    "    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421e8469-a54e-4cce-9f69-d91f016fee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## q: (seqLen, num_head, head_dim)\n",
    "## k: (seqLen, num_head, head_dim)\n",
    "## v: (seqLen, num_head, head_dim)\n",
    "def batch_prefill_baseline(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seqLens: List[int]):\n",
    "    attns = []\n",
    "    startingIndex = 0\n",
    "    for seqLen in seqLens:\n",
    "        seqSlice = slice(startingIndex, startingIndex + seqLen)\n",
    "        qs = q[seqSlice]\n",
    "        ks = k[seqSlice]\n",
    "        vs = v[seqSlice]\n",
    "        attns.append(prefill_single_seq_attn(qs, ks, vs, seqLen))\n",
    "        startingIndex += seqLen\n",
    "    return torch.cat(attns, dim=0) \n",
    "\n",
    "## q: (seqLen, num_head, head_dim)\n",
    "## k: (seqLen, num_head, head_dim)\n",
    "## v: (seqLen, num_head, head_dim)\n",
    "def prefill_single_seq_attn(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seqLen: int):\n",
    "    qt = q.transpose(1, 0) # (num_head, seqLen, head_dim)\n",
    "    kt = k.transpose(1, 0) # (num_head, seqLen, head_dim)\n",
    "    vt = v.transpose(1, 0) # (num_head, seqLen, head_dim)\n",
    "    scale = math.sqrt(kt.size(-1))\n",
    "\n",
    "    qkProduct = (qt @ kt.transpose(-2, -1)) * (1.0 / scale) # (num_head, seqLen, seqLen)\n",
    "    causalMask = torch.triu(torch.full((seqLen, seqLen), float('-inf'), dtype=dtype, device=device), diagonal=1) # lower triangular matrix\n",
    "    softmax = F.softmax(qkProduct + causalMask, dim=-1)\n",
    "    attn = softmax @ vt # (num_head, seqLen, seqLen) x (num_head, seqLen, head_dim) -> (num_head, seqLen, head_dim)\n",
    "    return attn.transpose(0,1) # (seqLen, num_head, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92904bfd-ede8-4000-b3aa-35c57ecf3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "## q: (numSeqs, num_head, head_dim)\n",
    "## k: (seqLen, num_head, head_dim)\n",
    "## v: (seqLen, num_head, head_dim)\n",
    "def batch_decode_baseline(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seqLens: List[int]):\n",
    "    attns = []\n",
    "    startingIndex = 0\n",
    "    for i, seqLen in enumerate(seqLens):\n",
    "        seqSlice = slice(startingIndex, startingIndex + seqLen)\n",
    "        qi = q[i] # rotary_embed(q[i], seqLen - 1) # (num_head, head_dim)\n",
    "        kSlice = k[seqSlice] # rotary_embed(k[seqSlice].transpose(0,1), 0).transpose(0,1) # (seqLen, num_head, head_dim)\n",
    "        attns.append(decode_single_seq_attn(qi, kSlice, v[seqSlice], seqLen))\n",
    "        startingIndex += seqLen\n",
    "    return torch.cat(attns, dim=0)\n",
    "\n",
    "def decode_single_seq_attn(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seqLen):\n",
    "    qt = q.view(num_head, 1, head_dim)  # (num_head, 1, head_dim)\n",
    "    kt = k.transpose(1, 0) # (num_head, seqLen, head_dim)\n",
    "    vt = v.transpose(1, 0) # (num_head, seqLen, head_dim)\n",
    "    scale = math.sqrt(head_dim)\n",
    "\n",
    "    qkProduct = (qt @ kt.transpose(-2, -1)) * (1.0 / scale) # (num_head, 1, seqLen)\n",
    "    # the causal mask is not needed for decoding\n",
    "    softmax = F.softmax(qkProduct, dim=-1)\n",
    "    attn = softmax @ vt # (num_head, 1, seqLen) x (num_head, seqLen, head_dim) -> (num_head, 1, head_dim)\n",
    "    return attn.transpose(0,1) # (1, num_head, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226fe6fb-d6ce-434a-91b4-2986e2f3db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvCache:\n",
    "    def __init__(self, start_page_idx: int, seq_len: int, page_size: int):\n",
    "        self.page_size = page_size\n",
    "        num_pages = math.ceil(seq_len / page_size)\n",
    "        self.kv_last_page_len = seq_len - (num_pages - 1) * page_size\n",
    "        self.kv_page_indices = [i for i in range(start_page_idx, start_page_idx + num_pages)]\n",
    "        self.kv_len = seq_len\n",
    "\n",
    "    def increment(self):\n",
    "        self.kv_len += 1\n",
    "        self.kv_last_page_len += 1\n",
    "        if self.kv_last_page_len > self.page_size:\n",
    "            self.kv_last_page_len -= self.page_size\n",
    "            self.kv_page_indices.append(self.kv_page_indices[-1] + 1)\n",
    "            \n",
    "\n",
    "class BatchedKvCache:\n",
    "    \"\"\"Key-value cache for a batch of sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, page_size: int, context_length: int, seq_lens: List[int]):\n",
    "        self.page_size = page_size\n",
    "        batch_size = len(seq_lens)\n",
    "        num_pages_per_seq = math.ceil(context_length / page_size)\n",
    "        total_num_pages = num_pages_per_seq * batch_size\n",
    "        self.kv_cache = torch.zeros(total_num_pages, 2, page_size, num_head, head_dim, dtype=dtype, device=device)\n",
    "        kvCacheList = []\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            start_page_idx = num_pages_per_seq * i\n",
    "            kvCacheList.append(KvCache(start_page_idx, seq_len, page_size))\n",
    "\n",
    "        self.kv_cache_pages_info = kvCacheList\n",
    "\n",
    "    def increment(self, kv_active: List[bool]):\n",
    "        for i, kvCache in enumerate(self.kv_cache_pages_info):\n",
    "            if kv_active[i]:\n",
    "                kvCache.increment()\n",
    "\n",
    "    def computeActiveKvData(self, kv_active: List[bool]):\n",
    "        kv_page_indices_list = []\n",
    "        kv_page_indptr_list = []\n",
    "        kv_last_page_len_list = []\n",
    "        cum_pages = 0\n",
    "        for i, kvCache in enumerate(self.kv_cache_pages_info):\n",
    "            if kv_active[i]:\n",
    "                kv_page_indices_list.extend(kvCache.kv_page_indices)\n",
    "                kv_page_indptr_list.append(cum_pages)\n",
    "                kv_last_page_len_list.append(kvCache.kv_last_page_len)\n",
    "                cum_pages += len(kvCache.kv_page_indices)\n",
    "\n",
    "        kv_page_indptr_list.append(cum_pages)\n",
    "        kv_page_indices = torch.tensor(kv_page_indices_list, dtype=torch.int32, device=device)\n",
    "        kv_page_indptr = torch.tensor(kv_page_indptr_list, dtype=torch.int32, device=device)\n",
    "        kv_last_page_len = torch.tensor(kv_last_page_len_list, dtype=torch.int32, device=device)\n",
    "        return kv_page_indices, kv_page_indptr, kv_last_page_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55973804-b11a-46e4-96a1-7ed62e3ba00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveKvCache:\n",
    "    \"\"\"Key-value cache to assist baseline attention computation \"\"\"\n",
    "    \n",
    "    def __init__(self, k: torch.tensor, v: torch.tensor, seqLens: List[int]):\n",
    "        batch_size = len(seqLens)\n",
    "        kList = [ None ] * batch_size\n",
    "        vList = [ None ] * batch_size\n",
    "        newKvIdx = 0\n",
    "        for i, seqLen in enumerate(seqLens):\n",
    "            kList[i] = k[newKvIdx: newKvIdx + seqLen]\n",
    "            vList[i] = v[newKvIdx: newKvIdx + seqLen]\n",
    "            newKvIdx += seqLen\n",
    "\n",
    "        self.kList = kList # per sequence key\n",
    "        self.vList = vList # per sequence value\n",
    "\n",
    "    def append(self, newK: torch.tensor, newV: torch.tensor, kv_active: List[bool]):\n",
    "        batch_size = len(kv_active)\n",
    "        newKvIdx = 0\n",
    "        for i, active in enumerate(kv_active):\n",
    "            if active:\n",
    "                self.kList[i] = torch.cat([self.kList[i], newK[newKvIdx: newKvIdx+1]], dim=0)\n",
    "                self.vList[i] = torch.cat([self.vList[i], newV[newKvIdx: newKvIdx+1]], dim=0)\n",
    "                newKvIdx += 1\n",
    "\n",
    "    def getActiveKvData(self, kv_active: List[bool]):\n",
    "        activeK = [ self.kList[i] for i, active in enumerate(kv_active) if active ]\n",
    "        activeV = [ self.vList[i] for i, active in enumerate(kv_active) if active ]\n",
    "        seqLens = [ k.data.shape[0] for i, k in enumerate(self.kList) if kv_active[i] ]\n",
    "        return torch.cat(activeK, dim=0), torch.cat(activeV, dim=0), seqLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7506a5-e761-4d0f-843b-5926b26f2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prefill_baseline_wrapper(q, naiveKvCache, kv_active):\n",
    "    k, v, seqLens = naiveKvCache.getActiveKvData(kv_active)\n",
    "    return batch_prefill_baseline(q, k, v, seqLens)   \n",
    "\n",
    "# hyperparameters\n",
    "page_size = 16 # number of tokens a page contains\n",
    "kv_layout = \"NHD\"\n",
    "causal = True\n",
    "pos_encoding_mode = \"NONE\"\n",
    "context_length = 32\n",
    "\n",
    "# sequence info\n",
    "seqLens = [3, 3, 3] # [18, 5, 22]\n",
    "batch_size = len(seqLens)\n",
    "totalSeqLen = sum(seqLens)\n",
    "\n",
    "# kvq initialization\n",
    "torch.manual_seed(0xABCDABCD987)\n",
    "k_prefill = torch.randn(totalSeqLen, num_head, head_dim, dtype=dtype, device=device)\n",
    "v_prefill = torch.randn(totalSeqLen, num_head, head_dim, dtype=dtype, device=device)\n",
    "q = torch.randn(totalSeqLen, num_head, head_dim, dtype=dtype, device=device)\n",
    "\n",
    "# batch length info\n",
    "qo_indptr = torch.cat(\n",
    "    [torch.zeros(1, dtype=torch.int32, device=device), torch.cumsum(torch.tensor(seqLens, dtype=torch.int32, device=device), dim=0)]\n",
    ").int()\n",
    "\n",
    "# kv cache allocation\n",
    "kv_active = [ seqLen > 0 for seqLen in seqLens ]\n",
    "batchKvCache = BatchedKvCache(page_size, context_length, seqLens)\n",
    "\n",
    "# move the kv data to cache\n",
    "naiveKvCache = NaiveKvCache(k_prefill, v_prefill, seqLens)\n",
    "kv_page_indices, kv_page_indptr, kv_last_page_len = batchKvCache.computeActiveKvData(kv_active)\n",
    "flashinfer.append_paged_kv_cache(\n",
    "    k_prefill,\n",
    "    v_prefill,\n",
    "    qo_indptr,\n",
    "    batchKvCache.kv_cache,\n",
    "    kv_page_indices,\n",
    "    kv_page_indptr,\n",
    "    kv_last_page_len)\n",
    "\n",
    "# compute prefill attention\n",
    "workspace_buffer = torch.empty(32 * 1024 * 1024, dtype=torch.int8).to(0)\n",
    "prefill_wrapper = flashinfer.BatchPrefillWithPagedKVCacheWrapper(\n",
    "    workspace_buffer, kv_layout\n",
    ")\n",
    "\n",
    "prefill_wrapper.begin_forward(\n",
    "    qo_indptr,\n",
    "    kv_page_indptr,\n",
    "    kv_page_indices,\n",
    "    kv_last_page_len,\n",
    "    num_head,\n",
    "    num_head,\n",
    "    head_dim,\n",
    ")\n",
    "\n",
    "flashinfer_attn = prefill_wrapper.forward(q, batchKvCache.kv_cache, causal=causal, pos_encoding_mode=pos_encoding_mode)\n",
    "baseline_attn = batch_prefill_baseline_wrapper(q, naiveKvCache, kv_active)\n",
    "assert_close(flashinfer_attn, baseline_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b0fd52-2643-418a-aafc-291a68705b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continously decoding\n",
    "def batch_decode_baseline_wrapper(q_decode, naiveKvCache, kv_active):\n",
    "    k, v, seqLens = naiveKvCache.getActiveKvData(kv_active)\n",
    "    return batch_decode_baseline(q_decode, k, v, seqLens)\n",
    "\n",
    "def batch_decode_flashinfer_wrapper(q_decode, batchKvCache, kv_active, workspace_buffer, kv_layout):\n",
    "    kv_page_indices, kv_page_indptr, kv_last_page_len = batchKvCache.computeActiveKvData(kv_active)\n",
    "    decode_wrapper = flashinfer.BatchDecodeWithPagedKVCacheWrapper(\n",
    "        workspace_buffer, kv_layout\n",
    "    )\n",
    "    decode_wrapper.begin_forward(\n",
    "        kv_page_indptr,\n",
    "        kv_page_indices,\n",
    "        kv_last_page_len,\n",
    "        num_head,\n",
    "        num_head,\n",
    "        head_dim,\n",
    "        page_size,\n",
    "        \"NONE\",\n",
    "        dtype,\n",
    "    )\n",
    "    return decode_wrapper.forward(q_decode, batchKvCache.kv_cache, pos_encoding_mode=pos_encoding_mode)    \n",
    "\n",
    "def append_kv_cache_wrapper(batchKvCache, kv_active):\n",
    "    batch_size_decode = sum(kv_active)\n",
    "    qo_decode_indptr = torch.arange(0, batch_size_decode + 1, dtype=torch.int32, device=device)\n",
    "    batchKvCache.increment(kv_active)\n",
    "    kv_page_indices, kv_page_indptr, kv_last_page_len = batchKvCache.computeActiveKvData(kv_active)\n",
    "    flashinfer.append_paged_kv_cache(\n",
    "        k_decode,\n",
    "        v_decode,\n",
    "        qo_decode_indptr,\n",
    "        batchKvCache.kv_cache,\n",
    "        kv_page_indices,\n",
    "        kv_page_indptr,\n",
    "        kv_last_page_len\n",
    "    )    \n",
    "\n",
    "for _ in range(5):\n",
    "    kv_active = [False, True, True] # the first sequence stopped, so only 2nd and 3rd ones are active\n",
    "    batch_size_decode = sum(kv_active)\n",
    "    k_decode = torch.randn(batch_size_decode, num_head, head_dim, dtype=dtype, device=device)\n",
    "    v_decode = torch.randn(batch_size_decode, num_head, head_dim, dtype=dtype, device=device)\n",
    "    q_decode = torch.randn(batch_size_decode, num_head, head_dim, dtype=dtype, device=device)\n",
    "\n",
    "    # decode\n",
    "    flashinfer_decode_attn = batch_decode_flashinfer_wrapper(q_decode, batchKvCache, kv_active, workspace_buffer, kv_layout)\n",
    "    baseline_decode_attn = batch_decode_baseline_wrapper(q_decode, naiveKvCache, kv_active)\n",
    "    assert_close(flashinfer_decode_attn, baseline_decode_attn)\n",
    "    \n",
    "    # append kv data to cache\n",
    "    naiveKvCache.append(k_decode, v_decode, kv_active)\n",
    "    append_kv_cache_wrapper(batchKvCache, kv_active)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
