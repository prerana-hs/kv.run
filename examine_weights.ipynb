{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db97e16a-acff-48d4-9065-cbd0a43e1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842784e4-03ca-4677-a284-0086c8b3b8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151331, 151333,   3838,    374,   5538,   6832,     30]],\n",
      "       device='cuda:0')\n",
      "torch.int64\n",
      "torch.Size([1, 7, 151552])\n",
      "torch.float16\n",
      "torch.Size([151552])\n"
     ]
    }
   ],
   "source": [
    "causal_input_ids = torch.load('causal_input_ids')\n",
    "causal_prefill_res = torch.load('causal_prefill_res')\n",
    "causal_logits = causal_prefill_res[0, -1, :]\n",
    "print(causal_input_ids)\n",
    "print(causal_input_ids.dtype)\n",
    "print(causal_prefill_res.shape)\n",
    "print(causal_prefill_res.dtype)\n",
    "print(causal_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f052cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  1724,   338,  6483,  6509, 29973], device='cuda:0')\n",
      "torch.Size([6, 32000])\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "flash_input_ids = torch.load('flash_input_ids')\n",
    "flash_prefill_res = torch.load('flash_prefill_out')\n",
    "flash_logits = flash_prefill_res[-1]\n",
    "print(flash_input_ids)\n",
    "print(flash_prefill_res.shape)\n",
    "print(flash_prefill_res.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5276260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([151331, 151333,   3838,    374,   5538,   6832,     30],\n",
      "       device='cuda:0')\n",
      "torch.Size([7, 151552])\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "flashinfer_input_ids = torch.load('flashinfer_input_ids')\n",
    "flashinfer_prefill_out = torch.load('flashinfer_prefill_out')\n",
    "flashinfer_logits = flashinfer_prefill_out[-1]\n",
    "print(flashinfer_input_ids)\n",
    "print(flashinfer_prefill_out.shape)\n",
    "print(flashinfer_prefill_out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1464452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a, b, rtol, atol):\n",
    "    # rtol, atol = {\n",
    "    #     torch.float16: (1e-2, 5e-2),\n",
    "    #     torch.bfloat16: (8e-3, 8e-3),\n",
    "    # }[a.dtype]\n",
    "    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0950da24",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 138115 / 151552 (91.1%)\nGreatest absolute difference: 15.4140625 at index (198,) (up to 0.03 allowed)\nGreatest relative difference: inf at index (84556,) (up to 0.1 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflashinfer_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(a, b, rtol, atol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_close\u001b[39m(a, b, rtol, atol):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# rtol, atol = {\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#     torch.float16: (1e-2, 5e-2),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     torch.bfloat16: (8e-3, 8e-3),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# }[a.dtype]\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/testing/_comparison.py:1523\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1501\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1502\u001b[0m     actual,\n\u001b[1;32m   1503\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1519\u001b[0m )\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 138115 / 151552 (91.1%)\nGreatest absolute difference: 15.4140625 at index (198,) (up to 0.03 allowed)\nGreatest relative difference: inf at index (84556,) (up to 0.1 allowed)"
     ]
    }
   ],
   "source": [
    "assert_close(causal_logits, flashinfer_logits, 1e-1, 3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95825fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_divergence(logits1, logits2):\n",
    "    sm1 = F.softmax(logits1, dim=1)\n",
    "    sm2 = F.softmax(logits2, dim=1)\n",
    "    kl_divergence_elementwise = F.kl_div(F.log_softmax(sm1, dim=1), sm2, reduction='none')\n",
    "    return torch.sum(kl_divergence_elementwise, dim=1)/sm1.size(1)\n",
    "\n",
    "\n",
    "def calc_divergence_single(sm1, sm2):\n",
    "    return F.kl_div(F.log_softmax(sm1), sm2, reduction='batchmean')\n",
    "\n",
    "def calc_divergence_single_log(logits1, logits2):\n",
    "    sm1 = F.softmax(logits1)\n",
    "    sm2 = F.softmax(logits2)\n",
    "    return F.kl_div(F.log_softmax(sm1), sm2, reduction='batchmean')\n",
    "\n",
    "# flash_prefill_sm = F.softmax(flash_prefill_res, dim=1)\n",
    "# flashinfer_prefill_sm = F.softmax(flashinfer_prefill_out, dim=1)\n",
    "# kl_divergence_elementwise = F.kl_div(F.log_softmax(flash_prefill_sm, dim=1), flashinfer_prefill_sm, reduction='none')\n",
    "# divergence = torch.sum(kl_divergence_elementwise, dim=1)/flash_prefill_sm.size(1)\n",
    "# print(divergence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32c559af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28828/3079652309.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sm1 = F.softmax(causal_logits)\n",
      "/tmp/ipykernel_28828/3079652309.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sm2 = F.softmax(flashinfer_logits)\n"
     ]
    }
   ],
   "source": [
    "sm1 = F.softmax(causal_logits)\n",
    "sm2 = F.softmax(flashinfer_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a45dc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 30 / 151552 (0.0%)\nGreatest absolute difference: 0.51220703125 at index (198,) (up to 0.003 allowed)\nGreatest relative difference: 10232.0 at index (198,) (up to 0.01 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43msm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msm2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(a, b, rtol, atol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_close\u001b[39m(a, b, rtol, atol):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# rtol, atol = {\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#     torch.float16: (1e-2, 5e-2),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     torch.bfloat16: (8e-3, 8e-3),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# }[a.dtype]\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/testing/_comparison.py:1523\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1501\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1502\u001b[0m     actual,\n\u001b[1;32m   1503\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1519\u001b[0m )\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 30 / 151552 (0.0%)\nGreatest absolute difference: 0.51220703125 at index (198,) (up to 0.003 allowed)\nGreatest relative difference: 10232.0 at index (198,) (up to 0.01 allowed)"
     ]
    }
   ],
   "source": [
    "assert_close(sm1, sm2, 1e-2, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c1c706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5122, 0.0469], device='cuda:0', dtype=torch.float16),\n",
       " tensor([5.0068e-05, 1.8001e-05], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1[[198, 8303]], sm2[[198, 8303]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01962f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5122, device='cuda:0', dtype=torch.float16),\n",
       " tensor(0.2363, device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm1.max(), sm2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "400defa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28828/3746471026.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sm1 = F.softmax(logits1)\n",
      "/tmp/ipykernel_28828/3746471026.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sm2 = F.softmax(logits2)\n",
      "/tmp/ipykernel_28828/3746471026.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.kl_div(F.log_softmax(sm1), sm2, reduction='batchmean')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.3690e-05, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_divergence_single_log(causal_logits, flashinfer_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f16b02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15778/2316408785.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.kl_div(F.log_softmax(sm1), sm2, reduction='batchmean')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2268)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts1 = torch.Tensor([0.1, 0.85, 0.05])\n",
    "ts2 = torch.Tensor([0.8, 0.15, 0.05])\n",
    "calc_divergence_single(ts1, ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85f9a855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.8000, 0.1000],\n",
       "        [0.1000, 0.8000, 0.1000]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([0.1, 0.8, 0.1]).repeat(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853aa629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
