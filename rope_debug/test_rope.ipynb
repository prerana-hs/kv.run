{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fab4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.load('cos')\n",
    "sin = torch.load('sin')\n",
    "\n",
    "key_layer_before_causal = torch.load(\"key_layer_before_causal\")\n",
    "query_layer_before_causal = torch.load(\"query_layer_before_causal\")\n",
    "key_layer_after_causal = torch.load(\"key_layer_after_causal\")\n",
    "query_layer_after_causal = torch.load(\"query_layer_after_causal\")\n",
    "\n",
    "key_layer_before_flashinfer = torch.load(\"key_layer_before_flashinfer\")\n",
    "query_layer_before_flashinfer = torch.load(\"query_layer_before_flashinfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb01ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_pos_emb_flashinfer = torch.cat((cos.transpose(1, 2), sin.transpose(1, 2)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f702575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rotary_emb\n",
    "def compute_rotary(x, cos, sin):\n",
    "    rotary_dim = cos.shape[-1] * 2\n",
    "    even_positions = list(range(0, rotary_dim, 2))\n",
    "    odd_positions = list(range(1, rotary_dim, 2))\n",
    "\n",
    "    k1 = x[..., even_positions]\n",
    "    k2 = x[..., odd_positions]\n",
    "    rotary_emb.apply_rotary(k1, k2, cos, sin, k1, k2, False)\n",
    "    x[..., :rotary_dim] = torch.stack((k1, k2), dim=-1).flatten(start_dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1702e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rotary(key_layer_before_flashinfer, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b5ff1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_layer_before_flashinfer.shape, key_layer_after_causal.shape\n",
    "assert_close(key_layer_before_flashinfer.transpose(0, 1), key_layer_after_causal[0], 1e-4, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31d43b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 7, 128]), torch.Size([7, 1, 32]), torch.Size([7, 1, 32]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_layer_before_causal.shape, cos.shape, sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb695661",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m k1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, even_positions]\n\u001b[1;32m      7\u001b[0m k2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, odd_positions]\n\u001b[0;32m----> 8\u001b[0m \u001b[43mrotary_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :rotary_dim] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((k1, k2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "rotary_dim = cos.shape[-1] * 2\n",
    "even_positions = list(range(0, rotary_dim, 2))\n",
    "odd_positions = list(range(1, rotary_dim, 2))\n",
    "x = key_layer_before_causal\n",
    "\n",
    "k1 = x[..., even_positions]\n",
    "k2 = x[..., odd_positions]\n",
    "rotary_emb.apply_rotary(k1, k2, cos, sin, k1, k2, False)\n",
    "x[..., :rotary_dim] = torch.stack((k1, k2), dim=-1).flatten(start_dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5f10c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 7, 32]),\n",
       " torch.Size([1, 2, 7, 32]),\n",
       " torch.Size([7, 1, 32]),\n",
       " torch.Size([7, 1, 32]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.shape, k2.shape, cos.shape, sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4742f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2793,  0.9600, -0.0879, -3.6035,  4.0000,  5.0000,  6.0000,\n",
       "           7.0000]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dim = 8\n",
    "rotary_dim = head_dim // 2\n",
    "rotary_dim_half = rotary_dim // 2\n",
    "\n",
    "cos1 = cos[6, 0, :rotary_dim_half].unsqueeze(0).unsqueeze(0)\n",
    "sin1 = sin[6, 0, :rotary_dim_half].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "k = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype = torch.float16, device=cos.device).unsqueeze(0).unsqueeze(0)\n",
    "compute_rotary(k, cos1, sin1)\n",
    "k\n",
    "# k1 = k[..., :rotary_dim_half]\n",
    "# k2 = k[..., rotary_dim_half : 2 * rotary_dim_half]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7765f203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2793,  0.9600, -0.0879, -3.6035]]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_positions + odd_positions\n",
    "torch.stack((k1, k2), dim=-1).flatten(start_dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4854d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0613b2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m k1\u001b[38;5;241m.\u001b[39mshape, cos1\u001b[38;5;241m.\u001b[39mshape, sin1\u001b[38;5;241m.\u001b[39mshape, \u001b[43mk2\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k2' is not defined"
     ]
    }
   ],
   "source": [
    "k1.shape, cos1.shape, sin1.shape, k2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a533ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.2793, -0.0879]], device='cuda:0', dtype=torch.float16),\n",
       " tensor([[ 0.9600, -3.6035]], device='cuda:0', dtype=torch.float16)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for pair in zip(k1, k2) for item in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fc4f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0., 1.],\n",
      "           [2., 3.]]]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2793,  0.9600, -0.0879, -3.6035,  4.0000,  5.0000,  6.0000,\n",
       "            7.0000]]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotary_pos_emb = torch.cat((cos1.transpose(1, 2), sin1.transpose(1, 2)), dim=2).unsqueeze(0)\n",
    "k_causal = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype = torch.float16, device=cos.device).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "apply_rotary_pos_emb(k_causal, rotary_pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0992812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 2]), torch.Size([1, 1, 1, 8]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotary_pos_emb.shape, k_causal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae763c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [b, np, sq, hn]\n",
    "    b, np, sq, hn = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "    rot_dim = rope_cache.shape[-2] * 2\n",
    "    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]\n",
    "    # truncate to support variable sizes\n",
    "    rope_cache = rope_cache[:, :sq]\n",
    "    xshaped = x.reshape(b, np, sq, rot_dim // 2, 2)\n",
    "    print(xshaped)\n",
    "    rope_cache = rope_cache.view(-1, 1, sq, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return torch.cat((x_out2, x_pass), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c07859a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]), tensor([[0, 2, 4]]), tensor([[2, 3]]), tensor([[1, 3, 5]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([0, 1, 2, 3, 4, 5]).reshape(3, 2).unsqueeze(0)\n",
    "t1[:, 0], t1[..., 0], t1[:, 1], t1[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c08fa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3],\n",
       "         [4, 5]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "610ce086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a, b, rtol, atol):\n",
    "    # rtol, atol = {\n",
    "    #     torch.float16: (1e-2, 5e-2),\n",
    "    #     torch.bfloat16: (8e-3, 8e-3),\n",
    "    # }[a.dtype]\n",
    "    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8369fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
